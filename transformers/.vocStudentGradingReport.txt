
[Executed at: Mon Dec 15 20:59:41 PST 2025]

test_attention_mask (__main__.TestTransformerComponents)
Test if attention masking works correctly ... ok
test_embeddings_implementation (__main__.TestTransformerComponents)
Test the implementation of token and position embeddings (TODO 4) ... ok
test_end_to_end_training (__main__.TestTransformerComponents)
Test end-to-end training of the model on real Shakespeare data ... ok
test_generation_implementation (__main__.TestTransformerComponents)
Test the implementation of the generation method (TODO 7) ... ok
test_head_implementation (__main__.TestTransformerComponents)
Test the implementation of the attention head (TODO 1) ... ok
test_layer_norm (__main__.TestTransformerComponents)
Test if layer normalization works correctly ... ok
test_loss_computation (__main__.TestTransformerComponents)
Test the implementation of the loss computation (TODO 6) ... ok
test_multihead_attention_implementation (__main__.TestTransformerComponents)
Test the implementation of multi-head attention (TODO 2) ... ok
test_transformer_block_implementation (__main__.TestTransformerComponents)
Test the implementation of the transformer block (TODO 3) ... ok
test_transformer_forward_implementation (__main__.TestTransformerComponents)
Test the implementation of the transformer forward pass (TODO 5) ... ok

----------------------------------------------------------------------
Ran 10 tests in 87.433s

OK


Summary:
Total tests: 10
Passed tests: 10
Failed tests: 0
Errors: 0
Final Grade: 60/60

Matplotlib is building the font cache; this may take a moment.

Running end-to-end training test (this will take a while)...
Initial losses: {'train': tensor(4.3974), 'val': tensor(4.3980), 'test': tensor(4.3900)}
step 0: train=4.1824, val=4.1752, test=4.1800
step 100: train=2.6649, val=2.6871, test=2.6659
step 200: train=2.4870, val=2.5238, test=2.5193
step 300: train=2.3928, val=2.4339, test=2.4236
step 400: train=2.3323, val=2.3707, test=2.3795
step 500: train=2.2777, val=2.3293, test=2.3201
step 600: train=2.2180, val=2.2700, test=2.2716
step 700: train=2.1684, val=2.2145, test=2.2289
step 800: train=2.1157, val=2.1663, test=2.1799
step 900: train=2.0879, val=2.1432, test=2.1635
Final losses after 1000 steps: {'train': tensor(2.0438), 'val': tensor(2.1207), 'test': tensor(2.1222)}

Generated text sample:

The athat these cword hopsetar Bull to not that ca
Training curves saved to 'training_curves.png'

Final Grade: 60/60
Passed 10/10 tests
----- Grading finished. Please check your grading report above to see if you failed on any test. -------
